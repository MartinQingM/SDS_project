{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SDS_REgroup_LOSO.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1H8YwLC4RQ1K7C4uSwH81kPEA-akwcdFF","authorship_tag":"ABX9TyPqMlMHKK6PInceOdy1/5aW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wWQAnRYxC6Et","colab_type":"code","outputId":"017e7fcf-6c62-4316-c960-2d24d732e76e","executionInfo":{"status":"ok","timestamp":1586205941338,"user_tz":240,"elapsed":2777,"user":{"displayName":"Ma Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjw-UPpKv64KV7UQkPfs5uPqCabqA72Hb0l-soQ=s64","userId":"10106945751744801708"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["import os\n","from IPython.display import clear_output\n","#!nvidia-smi\n","os.chdir(\"./drive/My Drive/Colab Notebooks/SDSProject/MyProject\")\n","os.getcwd()\n","os.listdir(\".\")"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Data',\n"," '.ipynb_checkpoints',\n"," 'temp',\n"," 'miccai18.ipynb',\n"," 'tempData',\n"," 'LearnResults',\n"," 'Regroup_LOUO',\n"," 'Regroup_LOSO',\n"," 'Regroup_LOSAUO',\n"," 'LOUO',\n"," 'LOSO',\n"," 'SDS_project_Learn.ipynb',\n"," 'SDS_LOSO.ipynb',\n"," 'SDS_REgroup_LOUO.ipynb',\n"," 'SDS_LOUO.ipynb',\n"," 'SDS_REgroup_LOSO.ipynb',\n"," 'SDS_DATAsetup_analysis.ipynb']"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"B7T1PNg6DAbT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"06753df6-74f6-44c8-cbbc-7d7b7e860d07","executionInfo":{"status":"ok","timestamp":1586205949097,"user_tz":240,"elapsed":10528,"user":{"displayName":"Ma Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjw-UPpKv64KV7UQkPfs5uPqCabqA72Hb0l-soQ=s64","userId":"10106945751744801708"}}},"source":["%tensorflow_version 1.x\n","!pip install Scikit-learn==0.19.1\n","clear_output()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Collecting Scikit-learn==0.19.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n","\u001b[K     |████████████████████████████████| 12.4MB 4.8MB/s \n","\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: Scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed Scikit-learn-0.19.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gSq6MyXugoxJ","colab_type":"code","outputId":"e004614c-58b9-4b61-d0e0-a1e543086a32","executionInfo":{"status":"ok","timestamp":1586205953765,"user_tz":240,"elapsed":15187,"user":{"displayName":"Ma Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjw-UPpKv64KV7UQkPfs5uPqCabqA72Hb0l-soQ=s64","userId":"10106945751744801708"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import imageio\n","import time, os, math, re, random, io, collections\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import keras\n","from itertools import chain\n","from keras.models import Model\n","from keras.utils import np_utils\n","from keras import regularizers\n","from keras.preprocessing import sequence\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","from sklearn.preprocessing import LabelEncoder\n","from mpl_toolkits.mplot3d import Axes3D"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mC1Gd-X0owYs","colab_type":"code","colab":{}},"source":["#File functions\n","#------------------------------------------------------#\n","#------------------Change I to E-----------------------#\n","#------------------------------------------------------#\n","def getExpertiseLevelOfSurgery(surgery_name):\n","  ## function getMetaDataForSurgeries should be already called\n","  if surgeries_metadata.__contains__(surgery_name):\n","    if surgeries_metadata[surgery_name][0] == 'I':\n","      return 'E'\n","    return surgeries_metadata[surgery_name][0]\n","  return None \n","\n","'''\n","def getExpertiseLevelOfSurgery(surgery_name):\n","\t## function getMetaDataForSurgeries should be already called\n","\tif surgeries_metadata.__contains__(surgery_name):\n","\t\treturn surgeries_metadata[surgery_name][0]\n","\treturn None \n","'''\n","\n","def getMetaDataForSurgeries(surgery_type):\n","\tsurgeries_metadata = {}\n","\tfile = open(root_dir+surgery_type+'/'+'meta_file_'+surgery_type+'.txt','r')\n","\tfor line in file: \n","\t\tline = line.strip() ## remove spaces\n","\t\n","\t\tif len(line)==0: ## if end of file\n","\t\t\tbreak\n","\n","\t\tb = line.split()\n","\t\tsurgery_name = b[0] \n","\t\texpertise_level = b[1]\n","\t\tb = b[2:]\n","\t\tscores = [int(e) for e in b]\n","\t\tsurgeries_metadata[surgery_name]=(expertise_level,scores)\n","\treturn surgeries_metadata\n","\n","def get_trial_num(surgery_name,surgery_type):\n","\ttrial_num = surgery_name.replace(surgery_type+'_',\"\")[-1]\n","\treturn trial_num\n","\n","def readFile(file_name,dtype,columns_to_use=None):\n","\tX = np.loadtxt(file_name,dtype,usecols=columns_to_use)\n","\treturn X\n","\n","\n","def generateMaps(surgery_type):\n","\tlistOfSurgeries =[]\n","\ty =[]\n","\tpath = root_dir+surgery_type+'/kinematics/AllGestures/'\n","\tfor subdir,dirs,files in os.walk(path):\n","\t\tfor file_name in files: \n","\t\t\tsurgery = readFile(path+file_name,float,columns_to_use=dimensions_to_use)\n","\t\t\tsurgery_name = file_name[:-4]\n","\t\t\texpertise_level = getExpertiseLevelOfSurgery(surgery_name)\n","\t\t\tif expertise_level is None: \n","\t\t\t\tcontinue\n","\t\t\tmapSurgeryDataBySurgeryName[surgery_name] = surgery\n","\t\t\tmapExpertiseLevelBySurgeryName[surgery_name] = expertise_level\n","\treturn None\n","\n","def fit_encoder(y_train,y_test,y_val): \n","\ty_train_test_val = y_train+y_test+y_val\n","\tencoder.fit(y_train_test_val)\n","\n","def convertStringClassesToBinaryClasses(y_train,y_test,y_val):\n","\tidx_y_test = len(y_train)\n","\tidx_y_val = len(y_train)+len(y_test)\n","\ty_train_test_val = y_train+y_test+y_val\n","\ty_train_test_val = encoder.transform(y_train_test_val)\n","\ty_train_test_val = np_utils.to_categorical(y_train_test_val)\n","\ty_train = y_train_test_val[0:idx_y_test]\n","\ty_test = y_train_test_val[idx_y_test:idx_y_val]\n","\ty_val = y_train_test_val[idx_y_val:]\n","\treturn y_train,y_test,y_val\n","\n","def write_csv_string_in_file(file_name,csv_string):\n","\tfile = open(path_to_results+ file_name + '.csv','w')\n","\tfile.write(csv_string)\n","\tfile.close()\n","\treturn True\n","\n","# shuffles train and labels \n","def shuffle(x_train,y_train):\n","\ty_train = np.array(y_train)\n","\ty_train = y_train.reshape(len(y_train),1)\n","\tx_train = x_train.reshape(len(x_train),1)\n","\tx_y_train = np.concatenate((x_train,y_train), axis=1)\n","\tnp.random.shuffle(x_y_train)\n","\treturn x_y_train[:,0] , x_y_train[:,1].tolist()\n","\n","#-------------------------------------------------------#\n","#-------------Supertrial out validation-----------------#\n","#-------------------------------------------------------#\n","def validation(surgery_type = 'Suturing' , summary=False, reg =0.01, max_itr=20):\n","\t# reg is the regularization parameter \n","\t# max_itr is the number of iterations to repeat the experiments\n","\tcounter = 0\n","\t#UserOut & SuperTrialOut\n","\tpath = path_to_configurations+surgery_type +'/'+'unBalanced'+'/'+'GestureClassification'+'/'+'SuperTrialOut'\n","\tresults = \"fold,iteration,macro,micro\\n\"\n","\tfor it in range(0,max_itr):\n","\t\tfor subdir,dirs,files in os.walk(path):\n","\t\t\t# One configuration with two files Train.txt and Test.txt\n","\t\t\tx_train = []\n","\t\t\ty_train = []\n","\t\t\tx_test = []\n","\t\t\ty_test = []\n","\t\t\tx_val = []\n","\t\t\ty_val = []\n","\t\t\ttrial_added_to_val = None\n","\t\t\tmin_length_train = np.iinfo(np.int32).max # this is the minimum length of a training instance\n","\t\t\tmin_length_test = np.iinfo(np.int32).max # this is the minimum length of a test instance\n","\t\t\tmin_length_val = np.iinfo(np.int32).max # this is the minimum length of a val instance\n","\t\t\tfor file_name in files:\n","\t\t\t\tdata = readFile(subdir+'/'+file_name,str)\n","\t\t\t\tsurgeries_set = set()\n","\t\t\t\tfor gesture in data:\n","\t\t\t\t\tsurgery_name = find_pattern(gesture[0],surgery_type+'_.00.')\n","\t\t\t\t\tsurgeries_set.add(surgery_name)\n","\t\n","\t\t\t\tfor surgery_name in surgeries_set:\n","\t\t\t\t\ttrial_num = get_trial_num(surgery_name,surgery_type)\n","\t\t\t\t\tif file_name == 'Train.txt':\n","\t\t\t\t\t\tif(trial_added_to_val is None):\n","\t\t\t\t\t\t\ttrial_added_to_val=trial_num\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tif(trial_num==trial_added_to_val): \n","\t\t\t\t\t\t\t# we should add to validation set \n","\t\t\t\t\t\t\tmin_length_val=min(len(mapSurgeryDataBySurgeryName[surgery_name]),min_length_val)\n","\t\t\t\t\t\t\tx_val.append(mapSurgeryDataBySurgeryName[surgery_name])\n","\t\t\t\t\t\t\ty_val.append(mapExpertiseLevelBySurgeryName[surgery_name])\n","\t\t\t\t\t\telse: # we add to the train set \n","\t\t\t\t\t\t\tmin_length_train = min(len(mapSurgeryDataBySurgeryName[surgery_name]),min_length_train)\n","\t\t\t\t\t\t\tx_train.append(mapSurgeryDataBySurgeryName[surgery_name])\n","\t\t\t\t\t\t\ty_train.append(mapExpertiseLevelBySurgeryName[surgery_name])\n","\t\t\t\t\telse:\n","\t\t\t\t\t\t# we are adding to the test set\n","\t\t\t\t\t\tmin_length_test = min(len(mapSurgeryDataBySurgeryName[surgery_name]),min_length_test)\n","\t\t\t\t\t\tx_test.append(mapSurgeryDataBySurgeryName[surgery_name])\n","\t\t\t\t\t\ty_test.append(mapExpertiseLevelBySurgeryName[surgery_name])\n","\t\t\t\t# end of one file Train or Test \n","\t\t\tif(len(files)>0):\n","\t\n","\t\t\t\tx_train = np.array(x_train)\n","\t\t\t\tx_test = np.array(x_test)\n","\t\t\t\tx_val = np.array(x_val)\n","\t\n","\t\t\t\tprint('train size:'+str(len(x_train)))\n","\t\t\t\tprint('val size:'+str(len(x_val)))\n","\t\t\t\tprint('test size:'+str(len(x_test)))\n","\t\n","\t\t\t\tfit_encoder(y_train,y_test,y_val)\n","\t\n","\t\t\t\tmodel = each_dim_build_model(input_shapes,summary=summary,reg =reg)\n","\t\t\t\t\n","\t\t\t\tfold = find_pattern(subdir,'SuperTrialOut'+'/.*_Out').replace('SuperTrialOut'+'/','').replace('_Out','')\n","\t\t\t\titeration = find_pattern(subdir, 'itr_.*').replace('itr_','')\n","\t\t\t\t# we train on each training instance \n","\t\t\t\t\t\n","\t\t\t\ty_test = fitModel(model,x_train,y_train,x_test,y_test,x_val,y_val)\t\n","\t\t\t\t\t\n","\t\t\t\tmodel = load_model('Regroup_LOSO/model.h5')# reload the best model saved \n","\n","\t\t\t\t# uncomment if you want to visualize the class activiation map as a gif \n","\t\t\t\t# generate_class_activation_map_for_all_surgeries(model,fold)\n","\t\t\t\t\n","\t\t\t\t# evaluate model and get results for confusion matrix \n","\t\t\t\t(macro,micro) = evaluateModel(model,x_test,y_test)\n","\t\t\t\tresults += fold+','+str(it)+','+str(macro)+','+str(micro)+'\\n'\n","\t\n","\t\t\t# end of one configuration \n","\tmatrix = confusion_matrix.to_numpy()\n","\tmacro = compute_macro(matrix)\n","\tmicro = compute_micro(matrix)\n","\tresults += 'total,total,'+str(macro)+','+str(micro)+'\\n'\n","\tresults_file_name = 'results'\n","\treturn write_csv_string_in_file(results_file_name,results)\n","\n","def find_pattern(word,pattern):\n","\treturn re.search(r''+pattern,word).group(0)\n","\n","def compute_micro(matrix):\n","\treturn sum(matrix.diagonal()) / np.sum(matrix)\n","\n","def compute_macro(matrix):\n","\tres = matrix.diagonal()/np.sum(matrix,axis=1)\n","\treturn np.nansum(res)/float(nb_classes)\n","\n","def fitModel(model,x_train,y_train,x_test,y_test,x_val,y_val):\n","\t# x_test and y_test are used to monitor the overfitting / underfitting not for training \n","\t# minimum epoch loss on val set\n","\tmin_val_loss =  np.iinfo(np.int32).max \n","\t# train for many epochs as specified by nb_epochs\n","\tfor epoch in range(0,nb_epochs) : \n","\t\t# shuffle before every epoch training \n","\t\tx_train,y_train=shuffle(x_train,y_train)\n","\t\t#convert string labels to binary forms\n","\t\ty_train_binary,y_test_binary,y_val_binary = convertStringClassesToBinaryClasses(y_train,y_test,y_val)\n","\t\t# train each sequence alone\n","\t\tepoch_val_loss = 0\n","\t\tfor sequence,label in zip(x_train,y_train_binary):\n","\t\t\tmodel.train_on_batch(split_input_for_training(sequence),label.reshape(1,nb_classes))\n","\t\t\t\n","\t\tepoch_val_loss = evaluate_for_epoch(model,x_val,y_val_binary)\n","\t\tif(epoch_val_loss < min_val_loss): # this is to choose finally the model that yields the best results on the validation set \n","\t\t\tmodel.save('Regroup_LOSO/model.h5')\n","\t\t\tmin_val_loss= epoch_val_loss\n","\treturn y_test_binary\n","\n","def evaluate_for_epoch(model,x_test,y_test):\n","\tepoch_test_loss = 0 \n","\tfor test,label in zip(x_test,y_test):\n","\t\tloss , acc = model.evaluate(split_input_for_training(test), label.reshape(1,nb_classes), verbose=0)\n","\t\tepoch_test_loss += loss ############### change if monitor acc instead of loss\n","\treturn epoch_test_loss/len(x_test)\n","\n","def evaluateModel(model,x_test,y_test):\n","\tconfusion_matrix_f = pd.DataFrame(np.zeros(shape = (nb_classes,nb_classes)), index = classes, columns = classes ) \n","\n","\tfor test,label in zip(x_test,y_test):\n","\t\tloss , acc = model.evaluate(split_input_for_training(test), label.reshape(1,nb_classes), verbose=0)\n","\t\tp = model.predict(split_input_for_training(test), batch_size = 1)\n","\t\tpredicted_integer_label = np.argmax(p).astype(int)\n","\t\tpredicted_label = encoder.inverse_transform(predicted_integer_label)\n","\t\tcorrect_label = encoder.inverse_transform(np.argmax(label))\n","\t\tconfusion_matrix[correct_label][predicted_label]+=1.0\n","\t\tconfusion_matrix_f[correct_label][predicted_label]+=1.0\n","\n","\tmatrix_f = confusion_matrix_f.to_numpy()\n","\tmacro = compute_macro(matrix_f)\n","\treturn (macro,compute_micro(matrix_f))\n","\n","# the sequence variable is the multivariate time series or in this case the surgical task\n","# we want to split the inputs in order to train  \n","def split_input_for_training(sequence):\n","\t# get number of hands \n","\tnum_hands= len(input_shapes)\n","\t# get number of dimensions cluster for each hand \n","\tnum_dim_clusters = len(input_shapes[0])\n","\t# define the new input sequence \n","\tx = []\n","\t# this is used to keep track of the assigned dimensions \n","\tlast = 0\n","\t# loop over each hand \n","\tfor i in range(num_hands):\n","\t\t# loop for each hand over the cluster of dimensions \n","\t\tfor j in range(num_dim_clusters): \n","\t\t\t# assign new input same length but different dimensions each time \n","\t\t\tx.append(np.array([sequence[:,last:(last+input_shapes[i][j][1])]]))\n","\t\t\t# remember last assigned \n","\t\t\tlast= input_shapes[i][j][1]\n","\t# return the new input \n","\treturn x                              \n","\n","def each_dim_build_model(input_shapes,summary=False, reg=0.00001): \n","\t# get number of hands \n","\tnum_hands= len(input_shapes)\n","\t# get number of dimensions cluster for each hand \n","\tnum_dim_clusters = len(input_shapes[0])\n","\t# first index for hand second for  dims\n","\tx =[[None for a in range(0,num_dim_clusters)]for b in range(num_hands)] \n","\t# first conv layer on each dim cluster for each hand \n","\tconv1 = [[None for a in range(0,num_dim_clusters)]for b in range(num_hands)] \n","\t# merged layers for each hand \n","\thand_layers =[None for a in range(num_hands)]\n","\t# second conv layer on concatenated conv1 for each hand\n","\tconv2 = [None for a in range(num_hands)] \n","\t# loop over each hand \n","\tfor i in range(0,num_hands): \n","\t\t# loop for each hand over the dimension (or channels) clusters \n","\t\tfor j in range(0,num_dim_clusters): \n","\t\t\t# input layer for each dimension cluster for each hand \n","\t\t\tx[i][j]=keras.layers.Input(input_shapes[i][j])\n","\t\t\t# first conv layer over the clustered dimensions or channels in terms of keras \n","\t\t\tconv1[i][j] = keras.layers.Conv1D(8,kernel_size=3,strides=1,padding='same', activity_regularizer=regularizers.l2(reg))(x[i][j])\n","\t\t\tconv1[i][j] = keras.layers.Activation('relu')(conv1[i][j])\n","\t\t# concatenate convolutions of first layer over the channels dimension for each hand \n","\t\thand_layers[i]=keras.layers.Concatenate(axis=-1)(conv1[i])\n","\t\t# do a second convolution over features extracted from the first convolution over each hand \n","\t\tconv2[i] = keras.layers.Conv1D(16,kernel_size=3, strides=1, padding='same', activity_regularizer=regularizers.l2(reg))(hand_layers[i])\n","\t\tconv2[i] = keras.layers.Activation('relu')(conv2[i])\n","\t# concatenate the features of the two hands \n","\tfinal_input = keras.layers.Concatenate(axis=-1)(conv2) \n","\t# do a final convolution over the features concatenated for all hands \n","\tconv3 = keras.layers.Conv1D(32,kernel_size=3,strides=1,padding='same', activity_regularizer = regularizers.l2(reg))(final_input)\n","\tconv3 = keras.layers.Activation('relu', name = \"conv_final\")(conv3)\n","\t# do a globla average pooling of the final convolution \n","\tpooled = keras.layers.GlobalAveragePooling1D()(conv3)\n","\t# add the final softmax classifier layer \n","\tout = keras.layers.Dense(nb_classes,activation='softmax')(pooled)\n","\t# create the model and link input to output \n","\tmodel = Model(inputs=list(chain.from_iterable(x)),outputs=out)\n","\t# show summary if specified \n","\tif summary==True : \n","\t\tmodel.summary()\n","\n","\t# choose the optimizer \n","\toptimizer = keras.optimizers.Adam()\n","\tmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","\treturn model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"789-BhjBhJuK","colab_type":"code","outputId":"658733e2-9483-4a6d-d843-ba1a6bd583f5","executionInfo":{"status":"ok","timestamp":1585585600683,"user_tz":240,"elapsed":7531071,"user":{"displayName":"Ma Martin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjw-UPpKv64KV7UQkPfs5uPqCabqA72Hb0l-soQ=s64","userId":"10106945751744801708"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["#time \n","start_time = time.time()\n","\n","# Global parameters \n","root_dir = os.getcwd()+'/Data/jigsaws/'\n","path_to_configurations = os.getcwd()+'/Data/jigsaws/Experimental_setup/'\n","path_to_results = os.getcwd()+'/Regroup_LOSO/'\n","nb_epochs = 1000\n","#s_types = ['Suturing', 'Knot_Tying', 'Needle_Passing']\n","surgery_type = 'Needle_Passing'\n","dimensions_to_use = range(0,76)\n","number_of_dimensions= len(dimensions_to_use)\n","input_shape = (None,number_of_dimensions) # input is used to specify the value of the second dimension (number of variables) \n","input_shapes = [[(None,3),(None,9),(None,3),(None,3),(None,1)],[(None,3),(None,9),(None,3),(None,3),(None,1)],[(None,3),(None,9),(None,3),(None,3),(None,1)],[(None,3),(None,9),(None,3),(None,3),(None,1)]]\n","# for each manipulator   x,y,z  ,rot matrx, x'y'z' , a'b'g' , angle  , ... same for the second manipulator ...   \n","\n","mapSurgeryDataBySurgeryName = collections.OrderedDict() # indexes surgery data (76 dimensions) by surgery name \n","mapExpertiseLevelBySurgeryName = collections.OrderedDict() # indexes exerptise level by surgery name \n","classes = ['N','E']\n","#classes = ['N','I','E']\n","nb_classes = len(classes)\n","confusion_matrix = pd.DataFrame(np.zeros(shape = (nb_classes,nb_classes)), index = classes, columns = classes ) # matrix used to calculate the JIGSAWS evaluation\n","encoder = LabelEncoder() # used to transform labels into binary one hot vectors \n","\n","surgeries_metadata = getMetaDataForSurgeries(surgery_type)\n","\n","generateMaps(surgery_type)\n","print('Number of different surgeries in total: '+str(len(mapSurgeryDataBySurgeryName)))\n","\n","# comment then uncommment the commented lines if you want to load a pre-trained model \n","\n","validation(surgery_type,reg = 0.00001,summary=False,max_itr=1)\n","\n","print(confusion_matrix)\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","print(\"End!\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of different surgeries in total: 28\n","train size:14\n","val size:6\n","test size:7\n","train size:15\n","val size:7\n","test size:5\n","train size:14\n","val size:7\n","test size:6\n","train size:16\n","val size:7\n","test size:4\n","train size:15\n","val size:7\n","test size:5\n","      N     E\n","N  10.0   0.0\n","E   0.0  17.0\n","--- 8437.875009536743 seconds ---\n","End!\n"],"name":"stdout"}]}]}